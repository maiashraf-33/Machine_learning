{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook content:**<br>  \n",
    "    - Decision tree classifier<br> \n",
    "    - Evaluate the classifier tree<br> \n",
    "    - Logistic regression VS classification tree<br> \n",
    "    - Using entropy as a criterion<br> \n",
    "    - Regression tree<br> \n",
    "    - Cross validation<br> \n",
    "    - Ensemble learning<br> \n",
    "    - Bagging and Random forest<br> \n",
    "    - Random forest regressor<br> \n",
    "    - Visualizing features importances<br> \n",
    "    - Adaboost classifier<br> \n",
    "    - Gradient boosting<br> \n",
    "    - Stochastic gradient boosting(SGB)<br> \n",
    "    - Tuning a CART's Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees are able to capture the non-linear relashionships between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wbc.csv')\n",
    "X = data[['radius_mean' ,'concave points_mean']]\n",
    "y = data['diagnosis'].map({'M':1, 'B':0})\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748743718592965"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 6, random_state= 42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_train)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8771929824561403\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8596491228070176"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how the logistic regression will perform with the same data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=1)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "accuracy_score(y_pred_logreg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847953216374269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dicision tree model with defining the criterion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv('auto.csv')\n",
    "encoded = pd.get_dummies(df.origin, prefix='origin')\n",
    "df = df.join(encoded)\n",
    "X = df.drop(['mpg', 'origin'], axis=1)\n",
    "y = df['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square root mean error:  3.703774914790968\n"
     ]
    }
   ],
   "source": [
    "# Regression example\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import numpy as np\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(max_depth= 8,\n",
    "                              min_samples_leaf= 0.13,\n",
    "                              random_state= 3)\n",
    "\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred = dt_reg.predict(X_test)\n",
    "mse_dt = mse(y_pred,y_test)\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "print('Square root mean error: ', rmse_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnose bias and variance problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error:  4.347965938549339\n",
      "Test error:  3.703774914790968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mse_cv =  - cross_val_score(dt_reg, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print('Train error: ', mse_cv.mean() ** 0.5)\n",
    "print('Test error: ', rmse_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf= 0.26, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Learning**<br>\n",
    "Voting classifier uses the same training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wbc.csv')\n",
    "X = data[['radius_mean' ,'concave points_mean']]\n",
    "y = data['diagnosis'].map({'M':1, 'B':0})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                   random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=1)\n",
    "knn = KNN(n_neighbors=27)\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=1)\n",
    "\n",
    "classifiers = [('Logistic regression',logreg),\n",
    "              ('K-neighbors', knn),\n",
    "              ('Dicision tree', dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression : 0.847953\n",
      "K-neighbors : 0.859649\n",
      "Dicision tree : 0.906433\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('{:s} : {:3f}'.format(clf_name, accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.8538011695906432\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier in sklearn\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print('Voting Classifier:' ,accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging**(Bootstrap aggregation)<br>\n",
    "Same algorithm trained on different subset of the data<br>\n",
    "Bagging has the effect of reducing the variance of individual models in the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bagging classifier : 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=1)\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of bagging classifier :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out Of Bag (OOB) instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63% of the training instances are sampled. The remaining 37% that are not sampled constitute what is known as the Out-of-bag or OOB instances.\n",
    "# OOB-score corresponds to the accuracy for classifiers and the r-squared score for regressors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.912\n",
      "OOB accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "                      n_estimators=300,\n",
    "                      oob_score=True, \n",
    "                      n_jobs=-1)\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_pred, y_test)\n",
    "obb_accuracy = bc.oob_score_\n",
    "print('Test accuracy: {:.3f}'.format(test_accuracy))\n",
    "print('OOB accuracy: {:.3f}'.format(obb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These results highlight how OOB-evaluation can be an efficient technique to obtain a performance estimate of a bagged-ensemble on unseen data without performing cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests (RF)**<br>\n",
    "- Random Forests is an ensemble method that uses a decision tree as a base estimator. <br>\n",
    "- Each estimator is trained on a different bootstrap sample having the same size as the <br>\n",
    "training set. Random forests introduces further randomization than bagging when training each of the base estimators. \n",
    "- Features are sampled at each node **without replacement**<br>\n",
    "- The node is splited using the sampled feature that maximizes information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('auto.csv')\n",
    "encoded = pd.get_dummies(df.origin, prefix='origin')\n",
    "df = df.join(encoded)\n",
    "X = df.drop(['mpg', 'origin'], axis=1)\n",
    "y = df['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 4.038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, \n",
    "                          min_samples_leaf=0.12,\n",
    "                          random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "rmse = mse(y_pred, y_test) ** 0.5\n",
    "print('Root mean square error: {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 4.206\n"
     ]
    }
   ],
   "source": [
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred = dt_reg.predict(X_test)\n",
    "rmse = mse(y_pred, y_test) ** 0.5\n",
    "print('Root mean square error: {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance in sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "important_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "sorted_importences = important_rf.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_importences.plot(kind='barh', color='c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting**<br>\n",
    "- AdaBoost(learn using weights)\n",
    "- Gradient Boosting(learn using residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "data = pd.read_csv('wbc.csv')\n",
    "X = data[['radius_mean' ,'concave points_mean']]\n",
    "y = data['diagnosis'].map({'M':1, 'B':0})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3,\n",
    "                                                   random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8816137566137566"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "ada_boost = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting (GB)\n",
    "df = pd.read_csv('auto.csv')\n",
    "encoded = pd.get_dummies(df.origin, prefix='origin')\n",
    "df = df.join(encoded)\n",
    "X = df.drop(['mpg', 'origin'], axis=1)\n",
    "y = df['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6122680865816617"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(max_depth=1, n_estimators=300, random_state=1)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_test)\n",
    "mse(y_test, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.489105675817158"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting (SGB)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Here, the parameter subsample was set to 0.8 in order for each tree to sample 80% of the data for training. \n",
    "# Finally, the parameter max_features was set to 0.2 so that each tree uses 20% of available features to perform the best-split. \n",
    "\n",
    "sgb = GradientBoostingRegressor(n_estimators=300,\n",
    "                                max_depth=1,\n",
    "                                max_features=0.2,\n",
    "                                subsample=0.8,\n",
    "                                random_state=1)\n",
    "\n",
    "sgb.fit(X_train, y_train)\n",
    "y_pred = sgb.predict(X_test)\n",
    "mse(y_test, y_pred) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning a CART's Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameters are learned from data through training; examples of parameters include the split-feature and the split-point of a node in a CART. \n",
    "- Hyperparameters are not learned from data; they should be set prior to training. Examples of hyperparameters include the maximum-depth and the splitting-criterion of a CART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# get all parameters of the decisiontree model\n",
    "print(dt.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wbc.csv')\n",
    "X = data[['radius_mean' ,'concave points_mean']]\n",
    "y = data['diagnosis'].map({'M':1, 'B':0})\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'max_features': 0.2, 'min_samples_leaf': 0.04}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "prams = {\n",
    "         'max_depth':[3, 4, 5],\n",
    "         'min_samples_leaf':[0.04, 0.06, 0.08],\n",
    "         'max_features':[0.2, 0.4, 0.6]\n",
    "        }\n",
    "grid_dt = GridSearchCV(estimator=dt, \n",
    "                       param_grid = prams, \n",
    "                       scoring='accuracy',\n",
    "                       cv=10,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "best_params = grid_dt.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929871794871795"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_dt.best_estimator_\n",
    "acc = best_model.score(X_test, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting RF Hyperparameters in sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1)\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "     'n_estimators':[300, 400, 500],\n",
    "     'max_depth':[4, 6, 8],\n",
    "     'min_samples_leaf':[0.1, 0.2],\n",
    "     'max_features':['log2','sqrt']\n",
    " }\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       cv=3,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   34.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=1), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'max_features': ['log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [0.1, 0.2],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4,\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_leaf': 0.1,\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22517315027353577\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_rf.best_estimator_\n",
    "y_pred = grid_rf.predict(X_test)\n",
    "rmse = mse(y_test, y_pred) ** 0.5\n",
    "print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
